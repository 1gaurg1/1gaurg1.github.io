<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.24">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>software – Gaurav Gaur</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dc55a5b9e770e841cd82e46aadbfb9b0.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-e31584831b205ffbb2d98406f31c2a5b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="styles.css">
<meta property="og:title" content="Gaurav Gaur">
<meta property="og:description" content="">
<meta property="og:image" content="https://1gaurg1.github.io/design docs/images/architecture.png">
<meta property="og:site_name" content="Gaurav Gaur">
<meta property="og:image:alt" content="d">
<meta property="og:image:height" content="854">
<meta property="og:image:width" content="1446">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="./index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Gaurav Gaur</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./hardware.html"> 
<span class="menu-text">Hardware Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link active" href="./software.html" aria-current="page"> 
<span class="menu-text">Software Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./research.html"> 
<span class="menu-text">Research</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#university-of-toronto-robotics-association-autonomous-rover-team-art" id="toc-university-of-toronto-robotics-association-autonomous-rover-team-art" class="nav-link active" data-scroll-target="#university-of-toronto-robotics-association-autonomous-rover-team-art">University of Toronto Robotics Association Autonomous Rover Team (ART)</a></li>
  <li><a href="#university-of-toronto-machine-intelligence-student-team" id="toc-university-of-toronto-machine-intelligence-student-team" class="nav-link" data-scroll-target="#university-of-toronto-machine-intelligence-student-team">University of Toronto Machine Intelligence Student Team</a></li>
  <li><a href="#deep-learning-facial-analysis-and-generation" id="toc-deep-learning-facial-analysis-and-generation" class="nav-link" data-scroll-target="#deep-learning-facial-analysis-and-generation">Deep Learning Facial Analysis and Generation</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<p>This section of my website is dedicated to my software projects, which demonstrate my skills in machine learning, data processing, and algorithm development. In addition, I showcase my skills in ROS and ROS2 for robotics control. These projects reflect my interest in cutting-edge technologies such as LLMs and other neural network architectures. It also showcases my ability to use advanced programming techniques, and also my dedication to solve complex problems in an innovative manner.</p>
<section id="university-of-toronto-robotics-association-autonomous-rover-team-art" class="level1">
<h1>University of Toronto Robotics Association Autonomous Rover Team (ART)</h1>
<p>On the hardware section of this website, I mention various hardware improvement efforts that I have led on the autonomous rover Espresso. My responsibilities, while rooted in hardware development, also include the core function of hardware-software integration. This involves robust implementation of ROS programming, simulations, controls, and odometry systems. A significant portion of this effort is directed toward maintaining and utilizing the simulation environment, encompassing tools like Gazebo, Cartographer, and SLAM (Simultaneous Localisation and Mapping). A critical task is the continuous maintenance and building of accurate URDF (Unified Robot Description Format) files. These URDFs need to be kinetically accurate to ensure the reliability of the simulations. Successfully managing this highly technical environment also requires the expertise to navigate the ROS packages efficiently and to promptly deal with version incompatibility issues that arise during development.</p>
<p>One key achievement in controls development involved diagnosing a fundamental mechanical flaw through software analysis, underscoring my ability to lead technical investigations. I successfully implemented and leveraged the motor odometry system. This involved integrating data from two distinct sources: the wheel encoder, which measured the rotation of the wheel, and the motor itself, which sends information about its own rotation. By piping this signal information into Python and performing Taylor approximation for numerical differentiation, I was able to quantify a significant discrepancy. The analysis revealed points where the wheel would stop but the motor would continue to spin, which is the textbook definition of mechanical slip. This diagnosis, made possible through detailed software analysis, is now leading rework efforts on the hardware side.</p>
<p>Currently, my focus is shifting toward systems modernization through a major migration to ROS 2 <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. Given that the legacy system only functioned on Ubuntu 20.04 and the older ROS distribution has reached End of Life (EOL), I am now working on migrating all systems to ROS 2 on Ubuntu 22.04. This transition provides extensive experience with both ROS and ROS 2, positioning me to apply this dual-system expertise in various future settings.</p>
</section>
<section id="university-of-toronto-machine-intelligence-student-team" class="level1">
<h1>University of Toronto Machine Intelligence Student Team</h1>
<p>Earlier, I was recruited by one of my connections into the University of Toronto Machine Intelligence Student Team <a href="https://utmist.gitlab.io/">(UTMIST)</a>. They are a student organization who coordinates many different machine learning projects and holds machine learning related workshops. Sometimes, they will liaise with external companies and research labs to give students the opportunity to work on ML/AI related projects. I am currently a member of one such team. We were contracted through UTMIST to improve an NLP system. Due to the nature of the project, I cannot go into details, but for the purposes of this portfolio I will provide some brief description. For similar reasons, there will also be no images in this section.</p>
<p>The first phase of the project had us generate large synthetic datasets of text. We attempted a few methods for this, including generating text embeddings with a simple GMM and implementation of an RAG (<span class="citation" data-cites="arslan2024survey">Arslan et al. (<a href="#ref-arslan2024survey" role="doc-biblioref">2024</a>)</span>). We experimented with many different architectures, including transformers and various adversarial training methods, but with the limited time and compute power we had we were unable to scale these up. Eventually, for scope reasons we settled on leveraging existing LLMs seeded with hand-tagged data. With some prompt engineering With some prompt engineering, we developed structured templates and dynamic context injection methods to improve the consistency and relevance of the generated text. This included defining hierarchical prompts that separated task instructions from contextual examples, as well as using few-shot exemplars to bias LLMs toward specific linguistic and semantic patterns present in our hand-tagged data. We also iteratively refined temperature and sampling parameters to balance diversity and coherence in the generated transcripts, and introduced post-processing filters to enforce domain-specific vocabulary and response structure. We became quite familiar with Microsoft’s Azure AI studio, and after some iteration we were able to scale up the size of the dataset by a large margin.</p>
<p>The second phase involves using the generated synthetic data to train a classifier. The classifier had to be deterministic, and must classify multiple non-exclusive labels. The labels were made based on psychological research (<span class="citation" data-cites="mayers2013predicting">Chapman and Mayers (<a href="#ref-mayers2013predicting" role="doc-biblioref">2013</a>)</span>). This is a non-trivial task, and initially, our goals were to leverage transfer learning wherever possible due to our computational restrictions and design a comprehensive classifier. While I cannot go into too many details about this portion, the team was effectivly able to utilize and fine tune transformers suited to long text sequences.</p>
</section>
<section id="deep-learning-facial-analysis-and-generation" class="level1 page-columns page-full">
<h1>Deep Learning Facial Analysis and Generation</h1>
<p>Our project <a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> focused on developing a deep learning model for detailed facial analysis and description. We aimed to create a system that could provide accurate descriptions of facial attributes, which could be useful in areas such as law enforcement, social media, and security, and recreate an image of a person based on footage. We recognised that traditional systems often struggle with unclear images, and the primary motivation of our project was overcoming this.</p>
<p>We used the <strong>CelebA dataset</strong>, which contains over 200,000 celebrity images, each annotated with 40 different attribute labels. We specifically used Convolutional Neural Networks (CNNs), because they are particularly good at identifying complex patterns in data, including facial attributes. We understood that creating an <strong>unbiased model</strong> was crucial for fairness, especially in law enforcement. We knew that we needed to continuously test and give feedback to the model so that it could improve over time.</p>
<p>We reviewed existing work in the area of facial recognition and description, noting the development of systems like DeepFace (<span class="citation" data-cites="taigman2014deepface">Taigman et al. (<a href="#ref-taigman2014deepface" role="doc-biblioref">2014</a>)</span>), which focused on identity verification using a 3D alignment process. We also looked at FairFace (<span class="citation" data-cites="karkkainen2021fairface">Karkkainen and Joo (<a href="#ref-karkkainen2021fairface" role="doc-biblioref">2021</a>)</span>), which addresses racial bias in datasets, and DebFace, which tackles bias in the feature extraction and classification stages of the model. We recognised that while off-the-shelf CNNs were useful, as shown by their use with the CelebA dataset, we needed to aim for a model that could do more than just identify faces, it needed to classify detailed attributes.</p>
<p>In terms of our model’s architecture, we developed both a <strong>classifier</strong> and a <strong>generator</strong>. The classifier is a CNN designed for image classification, which takes a 128x128x3 image as input and outputs 16 units, one for each classification label. The generator was designed to take a 1x16 tensor as input (representing the classification labels) and generate a 128x128x3 image. We used transposed convolutional layers, with leaky ReLU activations in between each, outputting a 128x128x32 feature map, followed by two normal convolutional layers to refine the image. Our model architecture most closely resembles that of an autoencoder [see figure 1].</p>
<div class="custom-style">
<p><img src="design docs/images/architecture.png" class="img-fluid" alt="d"> <!-- center image --></p>
<div class="caption text-center">
<p>Figure 1: Illustration of Model Architecture</p>
</div>
</div>
<p>For baseline models, we implemented <strong>transfer learning with pre-trained VGG-16</strong> for the classifier, and adapted a <strong>cGAN architecture designed for the MNIST dataset</strong> for the generator. We scaled down our images to 64x64 for the cGAN, and converted them to greyscale to better fit our limited resources. The results of the cGAN baseline model were not great, and outputted blurry, black blobs, however, tthey did retain the shape of a head proving the model was learning from the dataset. See figure 2 for an example output of the baseline cGAN.</p>

<div class="no-row-height column-margin column-container"><div class="">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="design docs/images/cgan.png" class="img-fluid figure-img"></p>
<figcaption>Figure 2: Output of Baseline cGAN</figcaption>
</figure>
</div>
</div></div><p>Our classifier model was trained using <strong>Hamming loss</strong>, and we achieved an average of 2/16 labels incorrectly predicted, which was an inaccuracy rate of 12.5%. The training curves of the classifier are shown below (figure 3).</p>
<div class="custom-style">
<p><img src="design docs/images/primary_train.png" class="img-fluid" alt="d"> <!-- center image --></p>
<div class="caption text-center">
<p>Figure 3: Illustration of Model Architecture</p>
</div>
</div>
<p>Our analysis found that our model is reasonably good at making predictions and reconstructions. We found that the model was unable to make faces in different positions, as it tended to standardise output to a forward facing view, which could be considered an advantage as it makes it easier to identify a suspect head-on. In addition, early versions of the model were prone to producing a checkered overlay due to improper hyperparameter tuning and the transposed convolutions (see figure 4). However, in the final build of the model, these were cleaned up and the model produced recognizable faces which resembled the original image fed into the classifier.</p>

<div class="no-row-height column-margin column-container"><div class="">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="design docs/images/checkering.png" class="img-fluid figure-img"></p>
<figcaption>Figure 4: Checkering in early builds</figcaption>
</figure>
</div>
</div></div><div class="custom-style">
<p><img src="design docs/images/outputs.png" class="img-fluid" alt="d"> <!-- center image --></p>
<div class="caption text-center">
<p>Figure 5: Final inputs and outputs of model</p>
</div>
</div>
<p>We also found the model had a bias towards makeup on women, and struggled to identify them if they were not wearing any. This is likely due to the dataset being biased towards photos of celebrities in public apprearances. Due to our classifier and generator being trained separately, the combined usage for data reconstruction was not ideal, but that was something we anticipated.</p>
<p>While we were able to produce something quite good given the constraints, now that the course is over I have many ideas on further work which could be done.</p>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-arslan2024survey" class="csl-entry" role="listitem">
Arslan, Muhammad, Hussam Ghanem, Saba Munawar, and Christophe Cruz. 2024. <span>“A Survey on RAG with LLMs.”</span> <em>Procedia Computer Science</em> 246: 3781–90.
</div>
<div id="ref-mayers2013predicting" class="csl-entry" role="listitem">
Chapman, Derek, and Dave Mayers. 2013. <span>“"Predicting Voluntary Turnover with Culture, Employee Values and Their Congruence".”</span> <em>Academy of Management Proceedings</em> 2013 (November): 16532–32. <a href="https://doi.org/10.5465/AMBPP.2013.16532abstract">https://doi.org/10.5465/AMBPP.2013.16532abstract</a>.
</div>
<div id="ref-karkkainen2021fairface" class="csl-entry" role="listitem">
Karkkainen, Kimmo, and Jungseock Joo. 2021. <span>“Fairface: Face Attribute Dataset for Balanced Race, Gender, and Age for Bias Measurement and Mitigation.”</span> In <em>Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</em>, 1548–58.
</div>
<div id="ref-taigman2014deepface" class="csl-entry" role="listitem">
Taigman, Yaniv, Ming Yang, Marc’Aurelio Ranzato, and Lior Wolf. 2014. <span>“Deepface: Closing the Gap to Human-Level Performance in Face Verification.”</span> In <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>, 1701–8.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p><a href="https://github.com/1gaurg1/Facial-Analysis-via-Deep-Learning">Github repository for facial analysis project</a><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p><a href="https://github.com/UTRA-ART/Espresso/tree/master">Github repository for autonomous rover espresso</a><a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/1gaurg1\.github\.io");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>